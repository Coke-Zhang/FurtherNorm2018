---
title: "Pre-clustering for scaling normalization"
author: Aaron Lun
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc_float: yes
bibliography: ref.bib
---

```{r, echo=FALSE, results="hide"}
knitr::opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE)
```

# Motivation

Common methods for scaling normalization assume that most genes are not differentially expressed (DE) between samples.
For specific methods such as that used by `r Biocpkg("DESEq2")`, this requires a non-DE majority of genes across all samples in the data set,
i.e., there is 50% of genes that exhibit no DE between any pair of samples.
This assumption is necessary due to the construction of an average pseudo-cell across all samples.
If every gene is DE in a subset of cells, the average pseudo-cell will include some contribution from DE for each gene.
The median ratio of each sample to the average pseudo-cell would no longer solely represent bias, but instead include some arbitrary DE.
(The only situation in which it would be okay is if the amount of DE is the same for each gene, such that the relative size factors are unchanged.
However, this seems unlikely.)

We introduced a pre-clustering step [@lun2016pooling] to weaken this assumption by reducing the amount of DE within each cluster.
This improves the accuracy of pseudo-cell-based scaling normalization when applied within each cluster.
We then rescale the size factors between clusters to ensure that they are comparable.
This rescaling process only requires pairwise normalization between clusters, and thus only requires an assumption of a non-DE majority between pairs of clusters.
For example, consider a situation where three clusters are present with a unique set of 20% DE genes each.
The non-DE majority assumption is satisfied between all pairs (40%) even if the global non-DE assumption is not (60%).
Even in relatively homogeneous populations, clustering will improve accuracy by avoiding skews in the distribution of the ratios, e.g., for median-based estimators.

# Choice of clustering algorithm

## Based on correlations

We originally used a clustering approach based on Spearman's rank correlation.
The aim was to ensure that the clustering was independent of the yet-to-be-removed scaling bias in each cell, as the correlations were not affected by scaling.

With hindsight, this was probably both ineffective and unnecessary.
It is ineffective as the number of zeroes and tied values is highly dependent on the scaling bias.
This means that samples with the same scaling bias cluster together, even with a correlation-based approach:

```{r}
set.seed(100)
ngenes <- 1000
prof <- runif(ngenes)
bias <- rep(c(0.5, 2), each=50)
true.means <- outer(prof, bias)
counts <- rpois(length(true.means), lambda=true.means)
dim(counts) <- dim(true.means)

library(scran)
sranks <- scaledColRanks(counts)
my.tree <- hclust(dist(t(sranks)))
table(bias, cutree(my.tree, 2))
table(bias, kmeans(t(sranks), 2)$cluster)
```

It is also unnecessary as the rescaling will remove all systematic differences in size factors between clusters (see below).
Thus, any effect of the scaling bias on clustering that manifests as a shift to the size factors will be removed anyway.

**Comments:**

- It is probably unwise to mean-center the genes before computing correlations (e.g., like PCA).
Two cells that are similar to the mean expression profile will have uncorrelated residuals and would appear to be unrelated. 
The correct correlation should be near unity as their expression values would match up perfectly. 

## Based on binarized data

Similar arguments apply to binarized data.
This is also invariant to scaling but is highly dependent on scaling bias, 
as the number of expressed features is tightly correlated to the library size for sparse single-cell data.
Thus, it is similarly ineffective as clustering based on the correlations.

```{r}
set.seed(1000)
ngenes <- 1000
prof <- runif(ngenes)
true.means <- outer(prof, rep(c(0.5, 2), each=50))
counts <- rpois(length(true.means), lambda=true.means)
dim(counts) <- dim(true.means)

by.bin <- as.matrix(dist(t(counts)>0))
my.tree <- hclust(dist(by.bin))
table(bias, cutree(my.tree, 2))
table(bias, kmeans(t(by.bin), 2)$cluster)
```

## Based on gene expression

Our current implementation in `quickCluster()` attempts to perform a mini-analysis in a single run.
It normalizes by library size, fits a trend to the endogenous genes and performs PCA with `denoisePCA()`.
This yields a set of PCs that are used for graph-based clustering with `buildSNNGraph()` and the Walktrap algorithm.

The aim is to use as much information to obtain as precise clusters as possible.
More appropriate clustering weakens the non-DE assumption and improves the accuracy of normalization.
By comparison, clusters based on correlations or binarized data tend to be coarser as they discard much information.

An interesting question is whether the library size factors are acceptable for this initial clustering step.
One could imagine that composition biases would yield distorted estimates of the normalized expression values and thus "incorrect" clusters.
However, this probably does not matter.
Strong composition biases are only introduced in the presence of strong DE between cells, and clustering should be obvious in such cases.

## Enforcing a minimum cluster size

We enforce a minimum cluster size to obtain stable pseudo-cells for accurate normalization within clusters and rescaling between clusters.
(For `computeSumFactors()`, this also increases the size of the linear system and the precision of the per-cell estimates.)
Previous versions of `quickCluster()` used a minimum size of 200 cells, but we have now reduced this to 100 cells.
This avoids inaccuracy from coercing multiple distinct clusters together and violating the non-DE assumption, at the cost of lower precision from insufficient cells.

For `method="hclust"`, the minimum cluster size is set using the arguments in `cutreeDynamic()`.
For `method="igraph"`, a custom greedy algorithm is used to guarantee all clusters are of the sufficient size.
If the smallest cluster contains fewer cells than `min.size`, it is merged with the cluster that maximizes modularity.
This process is repeated until all (merged) clusters are larger than `min.size`.

One could argue that even `min.size=100` is too high for applications with very heterogeneous populations.
However, coercion of two distinct clusters into one should not be an issue provided that the within-cluster normalization is robust to DE.
It is only a problem when there are multiple clusters that have many DE genes _and_ low numbers of cells.
This represents the pathological case where there is little information that can be sensibly shared across similar cells.
Hopefully, though, precise normalization would not be necessary in such cases where the DE is so strong.

# Rescaling between clusters

## Rationale

Each cluster has an average pseudo-cell to which the individual cells are normalized.
We then normalize the average pseudo-cells to a single reference pseudo-cell, chosen from one of the clusters (see below). 
We can use any pairwise scaling normalization strategy to do this - in `computeSumFactors()`, we compute the median ratio between the pseudo-cells across all genes.
This yields a "rescaling factor" that is used to rescale all size factors in the non-reference cluster.

The rescaling is justified by considering that the within-cluster normalization removes biases between each cell and the cluster-specific pseudo cell.
The $\tau$ represents the scaling between pseudo-cells of different clusters, to remove systematic biases between clusters.
Thus, by scaling all size factors by $\tau$, you eliminate differences between the cluster-specific pseudo cells.
This effectively means that all cells in all clusters are, now, scaled to the same pseudo cell.

In other words, the aim of the normalization is to remove systematic differences in coverage.
Conceptually, it doesn't matter whether you do this all at once, or remove differences within clusters first followed by removing differences between clusters.
The end result is still the same, i.e., differences are removed.
By comparison, pre-processing methods that use clustering to preserve differences in some manner (e.g., imputation) are inherently more suspect,
due to circularity when used with downstream clustering.

(Technically, the cluster-based re-scaling in `computeSumFactors()` refers to the normalization factors with the library size-adjusted expression values.
However, you would end up multiplying by the library size of each cell anyway to obtain the size factors.
There is no harm in multiplying by the library size first, and then scaling by $\tau$, given that the former has no effect on the calculation on the latter.)

## Choosing the reference cluster

The initial implementation of `computeSumFactors()` would choose the reference cluster based on that with the average library size closest to the median across all clusters.
This was done under the (rather weak) logic that this chosen cluster would be closest to the "middle" of the pseudo-cells.
Thus, it would have the least DE to each one, improving the accuracy of estimating the rescaling factors.

We have since modified this so that the reference cluster is that with the fewest zero entries.
This aims to identify the most stable pseudo-cell (either due to a cluster with many cells, or a cluster with greater coverage per cell) as the reference.
In this manner, we improve the precision of the rescaling factor estimates and reduce the risk of obtaining undefined rescaling factors.

Improved precision also improves accuracy of the rescaling factor estimates from median-based methods.
Reducing the variance of the non-DE ratios will reduce the absolute value of the shift in the median when DE is unbalanced.
This may yield more accurate rescaling factors, even if the number of DE genes is higher than another choice of reference.

## Nearest neighbour rescaling

It is tempting to think that we could use a nearest-neighbour strategy for rescaling.
Namely, if clusters $A$, $B$, $C$ and $D$ lie on a straight line, it would make sense that we would rescale the closest clusters first, e.g., $A$ to $B$.
We could then rescale $B$ to $C$, and then $C$ to $D$, taking advantage of the greater accuracy of median-based normalization when dealing with similar populations.

The problem is that, when we rescale $B$ to $C$, we also have to re-rescale $A$ using the $B\to C$ scaling factor.
This results in multiplicative errors that are almost as bad as rescaling $A$ to $C$ directly.
To illustrate, let's assume a normal model for the log-fold changes between clusters.
We can examine the error in using the median-based estimator as a function of the proportion of DE genes in one direction.

```{r}
p.de <- seq(0, 0.5, length.out=50)
p.nde <- 1 - p.de
error <- qnorm(p.nde * 0.5)
plot(p.de, error)
```

We observe that it is effectively linear, meaning that the error at 20% DE genes is twice that at 10% DE genes.
Consider a situation where $A$ and $B$ and $B$ and $C$ were separated by a different set of 10% DEGs.
The error from double rescaling of $A\to B$ and $B\to C$ would be equivalent to the error from just rescaling from $A\to C$ directly.

Of course, this is already the most optimistic scenario.
Consider instead a situation where $A$, $B$ and $C$ are separated by the same set of 10% DE genes, of varying levels of expression in each cluster.
Double rescaling would result in an effective 20% error from $A$ to $C$, even though a direct rescaling would only have 10%.

# Additional notes

- A subtle effect is there will be more all-zero and low-abundance genes in each cluster, due to semi-systematic zeroes specific to each cell type.
As such, it is important to re-filter _within_ each cluster to ensure that only informative genes are used within-cluster scaling normalization.

- An even more subtle effect is that the cells are no longer strictly independent.
Cells that are stochastically similar are more likely to be placed in the same cluster.
This weakens some properties of the pseudo-cell (stability from the law of large numbers, convergence to normality via the CLT; see `deconvolution.Rmd`). 
However, it would require very strong correlations between the majority of non-DE genes to have any impact.

- One might argue that we would not need robust scaling normalization strateiges at all if we were able to accurately cluster cells into their types.
Specifically, we could just normalize using the library size within each cluster, and then rescale robustly across clusters.
This is true, **provided you can cluster accurately in the first place** such that there are negligible DE genes within clusters.
The appeal of deconvolution is that it is robust to imprecisions in the clustering (especially in high dimensions) that retain some DE within each cluster.

# Session information

```{r}
sessionInfo()
```

# References

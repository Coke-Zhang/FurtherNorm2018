---
title: "Deconvolution for scaling normalization"
author: Aaron Lun
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc_float: yes
bibliography: ref.bib
---

```{r, echo=FALSE, results="hide"}
knitr::opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE)
```

# Overview

This document aims to provide a summary of the theory described in @lun2016pooling, with some corrections to the reasoning and additional practical notes.

# Normalizing the pools

## Framework 

Consider a non-DE gene $g$ with true abundance $\lambda_g$.
This can be in terms of number of transcript molecules or number of reads, depending on the technology - the interpretation of the absolute value is irrelevant at this point.
In cell $i$, we observe a count of $y_{ig}$ where
\[
    E(y_{ig}) =  \mu_{ig} = s_i\lambda_g
\]
in cell $i$ with scaling bias $s_i$.
We will scale this count with a cell-specific term $t_i^{-1}$, the reasons for which are discussed later.

For a pool $P$ of multiple cells, we obtain the pooled expression value $V_{gP}$ by summing $y_{ig}t_i^{-1}$ for all $i\in P$.
This gives us
\[
    E(V_{gP}) = \lambda_g \sum_{i\in\mathcal{P}} s_it_i^{-1}
\]

Finally, the average pseudo-cell is obtained by averaging across all $N$ cells in the dataset (or in the current cluster of interest, see `preclustering.Rmd`).
The averaged expression value is denoted as $U_g$ with $E(U_g) = \lambda_g C$ where $C$ is the mean of $s_it_i^{-1}$ across all cells.

## Computing the pool-based size factor

### Justifying the ratio

For each pool, we compute $V_{gP}/U_g$ for each gene.
We approximate the expectation of the ratio for gene $g$ with 

\[
    E\left(\frac{V_{gP}}{U_g}\right) \approx \frac{E(V_{gP})}{E(U_g)} = \frac{\sum_{i\in\mathcal{P}} s_it_i^{-1}}{ C } \;.
\]

In the manuscript, we justified this approximation using the law of large numbers.
The reasoning was that $U_g$ computed from many cells would be close to its expectation, 
such that its small variability could be ignored when considering the expectation of the ratio.
However, a better approach would be to consider a second-order Taylor expansion for the ratio of two random variables (see [here](www.stat.cmu.edu/~hseltman/files/ratio.pdf)).

\[
E\left(\frac{V_{gP}}{U_g}\right) \approx \frac{E(V_{gP})}{E(U_g)} - \frac{cov(V_{gP}, U_g)}{E(U_g)^2} + \frac{var(U_g)E(V_{gP})}{E(U_g)^3}
\]

In our setting, $U_g=(V_{gP}+X_{gP})/N$ where $X_{gP}$ is independent of $V_{gP}$ and represents the sum of expression values from cells not in $P$.
As a result,

\[
E\left(\frac{V_{gP}}{U_g}\right) \approx \frac{E(V_{gP})}{E(U_g)} - \frac{cov(V_{gP}, V_{gP}+X_{gP})}{E(U_g)^2N} + \frac{var(V_{gP}+X_{gP})E(V_{gP})}{E(U_g)^3N^2} \;.
\]

The error term then becomes 

\[
\begin{aligned}
& \frac{-var(V_{gP})}{E(U_g)^2N} + \frac{var(V_{gP}+X_{gP})E(V_{gP})}{E(U_g)^3N^2} \\
&= \frac{1}{E(U_g)^3N^2} [ var(V_{gP}+X_{gP})E(V_{gP}) - N var(V_{gP}) E(U_g) ] \\
&= \frac{1}{E(U_g)^3N^2} [ var(V_{gP})E(V_{gP}) + var(X_{gP})E(V_{gP}) - var(V_{gP})E(V_{gP}) - var(V_{gP})E(X_{gP}) ] \\
&= \frac{1}{E(U_g)^3N^2} [ var(X_{gP})E(V_{gP}) - var(V_{gP})E(X_{gP}) ]
\end{aligned}
\]

There are several features of this error term that make it small.
Firstly, it will tend to zero if $N$ is large, even after cancelling out the increase in $E(X_{gP})$ and $var(X_{gP})$ with an increased number of cells. 
Keep in mind that the moments of $V_{gP}$ are determined by the pool size and do not increase with the total number of cells.
(The size of $E(U_g)$ has less effect as any scaling cancels out between the denominator and numerator.)

Under stronger conditions, it is possible for the error term to become zero.
The error will be equal to zero if the expression values are Poisson-distibuted, as then $var(X_{gP})=E(X_{gP})$ and $var(V_{gP})=E(V_{gP})$.
This sounds reasonable but only works for UMI data and only when using $t_i=1$.
Alternatively, we could assume that all expression values are i.i.d. with mean $\mu$ and variance $\sigma^2$.
Assume that $P$ contains $N_P$ cells such that $X$ contains $N-N_P$ cells.
Thus,

\[
\begin{aligned}
E(V_{gP}) &= N_P \mu \;, \\
var(V_{gP}) &= N_P \sigma^2 \;, \\
E(X_{gP}) &= (N-N_P) \mu \; \mbox{and} \\
var(X_{gP}) &= (N-N_P) \sigma^2 \; \mbox{such that} \\
& \frac{1}{E(U_g)^3N^2} [ (N-N_P)N_P \mu\sigma^2 - (N-N_P)N_P \mu\sigma^2 = 0
\end{aligned}
\]

### Using the median

As we only have one observation per gene (per pool), it is not possible to compute the expectation of the ratio directly.
Rather, we exploit the fact that the expectation is the same across all non-DE genes.
By averaging across genes, we can obtain an estimate of the expected ratio

\[
\theta_P = \frac{\sum_{i\in\mathcal{P}} s_it_i^{-1}}{ C } \;.
\]

This represents a linear equation for each pool with the (estimated) pool-based size factor on the left and the linear combination of unknown $s_i$ on the right.

In practice, not all genes are non-DE.
Thus, we need a robust average that protects us from genes upregulated or downregulated in $P$ compared to the average pseudo-cell (large and small ratios, respectively).
The simplest approach is to take the median ratio across all genes.
We justify this by noting that the ratios should approach a normal distribution, based on the central limit theorem for $V_{gP}$ as a sum of expression values.
(This requires the additional assumption that $U_g$ is negligible in variance, which is probably reasonable for large pools.)
For a normal distribution, the median and mean are the same, so the former can be used as a robust proxy for the latter.
This seems to be the case even at low means with the smallest pool size:

```{r}
ngenes <- 10000
true.means <- rgamma(ngenes, 2, 2)
dispersions <- 0.1
blah <- matrix(rnbinom(ngenes*20, mu=true.means, size=1/dispersions), nrow=ngenes)
ratio <- rowSums(blah)/true.means
hist(ratio[ratio < 50], breaks=50, col="grey80")
```

The downside with the median is that it becomes increasingly inaccurate as we introduce a greater imbalance in the proportion of up/down-regulated DE genes.
This is because the median gets shifted away from the true non-DE mean in the presence of skews in the ratio distribution.
The inaccuracy is further exacerbated by our pooling procedure:

- Solution of the linear system "concentrates" the median bias into cells containing DE genes.
The simplest demonstration is that of a data set containing a 50:50 mix of two different types, each occupying one half of the library size-sorted ring.
When the sliding window hits the boundary of the two cell types, the median becomes inaccurate once the pool contains one cell of the other type.
However, the inaccuracy of the median is the same at the next position with two cells of the same type (as the same genes are DE, so the skew is unchanged).
This means that the bias is fully attributed to the first cell of the other type!
- The bias itself is relative to the pooled size factor.
Even if the relative error is small, the absolute error can become quite large for any given cell-specific size factor.
This is especially pronounced when the library size is not a good initial proxy for the size factor, see the issues related to $t_j$ choice below.

We have tried a number of alternatives to overcome the deficiencies of the median-based estimate, such as:

- Detecting the mode, based on a sliding window kernel of some kind.
- Fitting a truncated normal distribution to a trimmed distribution of the ratios, taking advantage of the CLT.
This involved using a maximum likelihood estimator for the mean and standard deviation, given the boundaries.
- Using the asymmetry in the distances between the first-second and second-third quartiles to perform correction,
exploiting the fact that this discrepancy between distances is about half the shift of the median from the true mean.
- Using the "shortest half" method (see [here](http://fmwww.bc.edu/RePEc/bocode/s/shorth.html)) to identify the location.

These methods were indeed less biased, but a lot less precise, e.g., variances were larger by at least an order of magnitude.
This resulted in unstable estimates of the pool-based size factors and greater errors in the final cell-based size factors.
As such, the median seems like the best choice despite its known flaws.

# Solving the linear system

## Overview

Repeating the estimation of the pool-based size factor $\theta_P$ for different pools yields a linear system where the variables are the unknown $s_i$.
Solving this system with standard least-squares methods (e.g., QR decomposition) will yield estimates for $s_i$, i.e., the cell-specific scaling factors.

Note that using least-squares in this application does not require normality or independence in the estimates of $\theta_P$.
Indeed, the QR approach simply involves multiplying the $\theta_P$ vector by Q and backsolving for the triangular matrix R.
This is equivalent to just adding or subtracting linear combinations of $\theta_P$ values.
Thus, the expected value of each estimate should not be affected by the distribution or correlations.

However, the standard error will be affected by correlations.
The residuals are correlated across equations due to the fact that the pooled size factors are computed from the same counts.
Thus, we probably shouldn't be estimating the standard error from the fit, contrary to the claims in @lun2016pooling.

## Defining the pools

## Using a library size-sorted ring

All libraries are arranged onto a ring in order of increasing library size as one goes clockwise from 6 to 12 o'clock and then decreasing library size from 12 to 6.
We apply a sliding window of size $s$ onto this ring.
Each location of the sliding window defines a pool from which a linear equation is obtained.
This yields $N$ linear equations in total after sliding the window around the ring, where each library is involved in $s$ equations.

The real benefit for sorting by library size is different to that described in @lun2016pooling.
Specifically, cells with small library size will have more discrete counts and low $t_i$.
This results in large, discrete $V_{gP}$ values after library size adjustment.
To improve the precision of the median ratio estimate, we need to reduce discreteness, and this can only be done by adding together values of similar size.
This motivates the use of the library size-sorted ring to group cells with similar "levels" of discreteness together.

There is no point adding $V_{gP}$ from large and small cells, as the discreteness from the latter will effectively manifest as DE and skew the pooled size factor estimate.
Convergence to a normal distribution or low-dispersion NB distribution would be compromised.
Indeed, randomly scattering these cells around the ring will reduce the accuracy of all cells, large and small (see [`poolsim.Rout.save`](https://github.com/MarioniLab/Deconvolution2016/blob/master/simulations/poolsim.Rout.save)).
It's not just a matter of the small cells having larger variance.
If it were, one would expect that random placement would actually improve the accuracy of small cells (as they get pooled with the more precise large cells).

## Why use different pool sizes?

The use of multiple different pool sizes in `computeSumFactors()` improves precision (see [`standerr.Rout.save`](https://github.com/MarioniLab/Deconvolution2016/blob/master/simulations/standerr.Rout.save)).
This occurs for several reasons:

- The inefficiency of the median estimator means that even nested pools provide extra information.
By comparison, had the mean been used, changing the number of pool sizes used would not make any difference.
- Use of multiple pools increases column rank and reduces imprecision due to unestimable coefficients.
It would be very unfortunate for any linear system to have common factors with all pool sizes (see identifiability concerns below).
- The use of large pools provides robustness to low counts, which compromises the use of median in small pools.
Conversely, the use of small pools ensures that libraries with similar count magnitudes are added together (see CLT concerns above).

The minimum pool size is chosen to reduce the probability of zeros in $V_{gP}$.
With just 20 cells, it's possible to have over 95% randomly distributed zeros and still get a non-zero pooled size factor ($0.95^{20} < 0.5$).
In practice, the tolerable proportion of zeroes is probably lower because some zeros are semi-systematic and you need multiple non-zero counts for the median to work.
Smaller cells will also have more stochastic zeros anyway.
Nonetheless, the equivalent failure point for DESeq would be 50%, which is clearly worse.

## Ensuring identifiability

## Dealing with negative size factors

# References

---
title: "Deconvolution for scaling normalization"
author: Aaron Lun
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc_float: yes
bibliography: ref.bib
---

```{r, echo=FALSE, results="hide"}
knitr::opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE)
```

# Overview

This document provides a summary of the theory behind the deconvolution approach, as described in @lun2016pooling.
It includes some corrections and clarifications to the reasoning as well as some additional practical notes.
We will assume that the reader is already familiar with the concept of composition biases in scaling normalization of genomics data.

The mathematical framework from the original paper is summarized here.
Consider a non-DE gene $i$ with true abundance $\lambda_{i0}$.
This can be in terms of number of transcript molecules or number of reads, depending on the technology - the interpretation of the absolute value is irrelevant at this point.
In cell $j$ with a scaling bias $\theta_j$, we observe a count of $y_{ij}$ where $E(y_{ij}) =  \lambda_{i0}\theta_j$.
We will scale this count with a cell-specific term $t_j^{-1}$, the reasons for which are discussed later.

Consider a pool $k$ consisting of an arbitrary set of cells $\mathcal{S}_k$.
We obtain the pooled expression value $V_{ik}$ by summing $y_{ij}t_j^{-1}$ for all $j\in \mathcal{S}_k$.
This gives us
\[
    E(V_{ik}) = \lambda_{i0} \sum_{j\in\mathcal{S}_k} \theta_jt_j^{-1}
\]

Finally, the average pseudo-cell is obtained by averaging across all $N$ cells in the dataset (or in the current cluster of interest, see `preclustering.Rmd`).
The averaged expression value is denoted as $U_i$ with $E(U_i) = \lambda_{i0} C$ where $C$ is the mean of $\theta_jt_j^{-1}$ across all cells.

# Normalizing the pools

## Computing the ratio

For each pool, we compute $V_{ik}/U_i$ for each gene.
We approximate the expectation of the ratio for gene $i$ with 

\[
    E\left(\frac{V_{ik}}{U_i}\right) \approx \frac{E(V_{ik})}{E(U_i)} = \frac{\sum_{j\in\mathcal{S}_k} \theta_jt_j^{-1}}{ C } \;.
\]

In the manuscript, we justified this approximation using the law of large numbers.
The reasoning was that $U_i$ computed from many cells would be close to its expectation, 
such that its small variability could be ignored when considering the expectation of the ratio.
However, a better approach would be to consider a second-order Taylor expansion for the ratio of two random variables (see [here](www.stat.cmu.edu/~hseltman/files/ratio.pdf)).

\[
E\left(\frac{V_{ik}}{U_i}\right) \approx \frac{E(V_{ik})}{E(U_i)} - \frac{cov(V_{ik}, U_i)}{E(U_i)^2} + \frac{var(U_i)E(V_{ik})}{E(U_i)^3}
\]

In our setting, $U_i=(V_{ik}+X_{ik})/N$ where $X_{ik}$ is independent of $V_{ik}$ and represents the sum of expression values from cells not in $P$.
As a result,

\[
E\left(\frac{V_{ik}}{U_i}\right) \approx \frac{E(V_{ik})}{E(U_i)} - \frac{cov(V_{ik}, V_{ik}+X_{ik})}{E(U_i)^2N} + \frac{var(V_{ik}+X_{ik})E(V_{ik})}{E(U_i)^3N^2} \;.
\]

The error term then becomes 

\[
\begin{aligned}
& \frac{-var(V_{ik})}{E(U_i)^2N} + \frac{var(V_{ik}+X_{ik})E(V_{ik})}{E(U_i)^3N^2} \\
&= \frac{1}{E(U_i)^3N^2} [ var(V_{ik}+X_{ik})E(V_{ik}) - N var(V_{ik}) E(U_i) ] \\
&= \frac{1}{E(U_i)^3N^2} [ var(V_{ik})E(V_{ik}) + var(X_{ik})E(V_{ik}) - var(V_{ik})E(V_{ik}) - var(V_{ik})E(X_{ik}) ] \\
&= \frac{1}{E(U_i)^3N^2} [ var(X_{ik})E(V_{ik}) - var(V_{ik})E(X_{ik}) ]
\end{aligned}
\]

There are several features of this error term that make it small.
Firstly, it will tend to zero if $N$ is large, even after cancelling out the increase in $E(X_{ik})$ and $var(X_{ik})$ with an increased number of cells. 
Keep in mind that the moments of $V_{ik}$ are determined by the pool size and do not increase with the total number of cells.
(The size of $E(U_i)$ has less effect as any scaling cancels out between the denominator and numerator.)

Under stronger conditions, it is possible for the error term to become zero.
The error will be equal to zero if the expression values are Poisson-distibuted, as then $var(X_{ik})=E(X_{ik})$ and $var(V_{ik})=E(V_{ik})$.
This sounds reasonable but only works for UMI data and only when using $t_i=1$.
Alternatively, we could assume that all expression values are i.i.d. with mean $\mu$ and variance $\sigma^2$.
Assume that $\mathcal{S}_k$ contains $N_k$ cells such that $X$ contains $N-N_k$ cells.
Thus,

\[
\begin{aligned}
E(V_{ik}) &= N_k \mu \;, \\
var(V_{ik}) &= N_k \sigma^2 \;, \\
E(X_{ik}) &= (N-N_k) \mu \; \mbox{and} \\
var(X_{ik}) &= (N-N_k) \sigma^2 \; \mbox{such that} \\
& \frac{1}{E(U_i)^3N^2} [ (N-N_k)N_k \mu\sigma^2 - (N-N_k)N_k \mu\sigma^2 = 0
\end{aligned}
\]

## Using the median

As we only have one observation per gene (per pool), it is not possible to compute the expectation of the ratio directly.
Rather, we exploit the fact that the expectation is the same across all non-DE genes.
By averaging across genes, we can obtain an estimate of the expected ratio

\[
\theta_k = \frac{\sum_{j\in\mathcal{S}_k} \theta_jt_j^{-1}}{ C } \;.
\]

This represents a linear equation for each pool with the (estimated) pool-based size factor on the left and the linear combination of unknown $s_i$ on the right.

In practice, not all genes are non-DE.
Thus, we need a robust average that protects us from genes upregulated or downregulated in $P$ compared to the average pseudo-cell (large and small ratios, respectively).
The simplest approach is to take the median ratio across all genes.
We justify this by noting that the ratios should approach a normal distribution, based on the central limit theorem for $V_{ik}$ as a sum of expression values.
(This requires the additional assumption that $U_i$ is negligible in variance, which is probably reasonable for large pools.)
For a normal distribution, the median and mean are the same, so the former can be used as a robust proxy for the latter.
This seems to be the case even at low means with the smallest pool size:

```{r}
ngenes <- 10000
true.means <- rgamma(ngenes, 2, 2)
dispersions <- 0.1
blah <- matrix(rnbinom(ngenes*20, mu=true.means, size=1/dispersions), nrow=ngenes)
ratio <- rowSums(blah)/true.means
hist(ratio[ratio < 50], breaks=50, col="grey80")
```

The downside with the median is that it becomes increasingly inaccurate as we introduce a greater imbalance in the proportion of up/down-regulated DE genes.
This is because the median gets shifted away from the true non-DE mean in the presence of skews in the ratio distribution.
The inaccuracy is further exacerbated by our pooling procedure:

- Solution of the linear system "concentrates" the median bias into cells containing DE genes.
The simplest demonstration is that of a data set containing a 50:50 mix of two different types, each occupying one half of the library size-sorted ring.
When the sliding window hits the boundary of the two cell types, the median becomes inaccurate once the pool contains one cell of the other type.
However, the inaccuracy of the median is the same at the next position with two cells of the same type (as the same genes are DE, so the skew is unchanged).
This means that the bias is fully attributed to the first cell of the other type!
- The bias itself is relative to the pooled size factor.
Even if the relative error is small, the absolute error can become quite large for any given cell-specific size factor.
This is especially pronounced when the library size is not a good initial proxy for the size factor, see the issues related to $t_j$ choice below.

We have tried a number of alternatives to overcome the deficiencies of the median-based estimate, such as:

- Detecting the mode, based on a sliding window kernel of some kind.
- Fitting a truncated normal distribution to a trimmed distribution of the ratios, taking advantage of the CLT.
This involved using a maximum likelihood estimator for the mean and standard deviation, given the boundaries.
- Using the asymmetry in the distances between the first-second and second-third quartiles to perform correction,
exploiting the fact that this discrepancy between distances is about half the shift of the median from the true mean.
- Using the "shortest half" method (see [here](http://fmwww.bc.edu/RePEc/bocode/s/shorth.html)) to identify the location.

These methods were indeed less biased, but a lot less precise, e.g., variances were larger by at least an order of magnitude.
This resulted in unstable estimates of the pool-based size factors and greater errors in the final cell-based size factors.
As such, the median seems like the best choice despite its known flaws.

## Pre-scaling by the library size

To be more precise, the problem with large libraries is that they have larger variance in their counts.
This manifests as large errors in the pooled size factors that will propagate throughout the linear system.
Subsequently, the size factor estimates for the smaller libraries will have very large relative errors, even though the absolute errors may be of the same size.

Prescaling by $t_j$ aims to get all of the fitted values of the linear system as similar to each other as possible.
This ensures that the absolute error is equivalent to the relative error (upon reversing the scaling by $t_j$).
Our assumption is that the library size is close to the true size factor, so we set $t_j$ to the library size.
This demonstrably improves the stability of the estimates for small cells and reduces the chance of obtaining negative size factor estimates.

Of course, when this assumption is not true, we can get very large relative errors as the fitted values are very unequal. 
This is most apparent in pathological situations where there are many DE genes and the library sizes are also very different.
Any absolute errors (due to the bias of the median) are subsequently amplified into large relative errors.
This can be mitigated by using the newly computed size factors (which are presumably more accurate than the library sizes) as $t_j$ in a second call to `computeSumFactors()`.

See `simulations/justifications/prescaling.Rmd` for a demonstration of these effects.

# Solving the linear system

## Overview

Repeating the estimation of the pool-based size factor $\theta_k$ for different pools yields a linear system where $\theta_jt_j^{-1}$ are the coefficients.
Solving this system with standard least-squares methods (e.g., QR decomposition) will yield estimates for $\theta_jt_j^{-1}$.
We scale by the known $t_j$ to obtain estimates for $\theta_j$, i.e., the cell-specific scaling factors.

Note that using least-squares in this application does not require normality or independence in the estimates of $\theta_k$.
Indeed, the QR approach simply involves multiplying the $\theta_k$ vector by Q and backsolving for the triangular matrix R.
This is equivalent to just adding or subtracting linear combinations of $\theta_k$ values.
Thus, the expected value of each estimate should not be affected by the distribution or correlations.

However, the standard error will be affected by correlations.
The residuals are correlated across equations due to the fact that the pooled size factors are computed from the same counts.
Thus, we probably shouldn't be estimating the standard error from the fit, contrary to the claims in @lun2016pooling.

## Defining the pools

## Using a library size-sorted ring

All libraries are arranged onto a ring in order of increasing library size as one goes clockwise from 6 to 12 o'clock and then decreasing library size from 12 to 6.
We apply a sliding window of size $s$ onto this ring.
Each location of the sliding window defines a pool from which a linear equation is obtained.
This yields $N$ linear equations in total after sliding the window around the ring, where each library is involved in $s$ equations.

The real benefit for sorting by library size is different to that described in @lun2016pooling.
Specifically, cells with small library size will have more discrete counts and low $t_i$.
This results in large, discrete $V_{ik}$ values after library size adjustment.
To improve the precision of the median ratio estimate, we need to reduce discreteness, and this can only be done by adding together values of similar size.
This motivates the use of the library size-sorted ring to group cells with similar "levels" of discreteness together.

There is no point adding $V_{ik}$ from large and small cells, as the discreteness from the latter will effectively manifest as DE and skew the pooled size factor estimate.
Convergence to a normal distribution or low-dispersion NB distribution would be compromised.
Indeed, randomly scattering these cells around the ring will reduce the accuracy of all cells, large and small (see `simulations/justification/poolsim.R`).
It's not just a matter of the small cells having larger variance.
If it were, one would expect that random placement would actually improve the accuracy of small cells (as they get pooled with the more precise large cells).

The use of the library size-sorted ring also provides some measure of robustness to an arbitrary input order of cells.
However, this fails as a standardization strategy in the presence of tied library sizes.

## Why use different pool sizes?

The use of multiple different pool sizes in `computeSumFactors()` improves precision (see `simulations/justifications/standerr.R`).
This occurs for several reasons:

- The inefficiency of the median estimator means that even nested pools provide extra information.
By comparison, had the mean been used, changing the number of pool sizes used would not make any difference.
- Use of multiple pools increases column rank and reduces imprecision due to unestimable coefficients.
It would be very unfortunate for any linear system to have common factors with all pool sizes (see identifiability concerns below).
- The use of large pools provides robustness to low counts, which compromises the use of median in small pools.
Conversely, the use of small pools ensures that libraries with similar count magnitudes are added together (see CLT concerns above).

The minimum pool size is chosen to reduce the probability of zeros in $V_{ik}$.
With just 20 cells, it's possible to have over 95% randomly distributed zeros and still get a non-zero pooled size factor ($0.95^{20} < 0.5$).
In practice, the tolerable proportion of zeroes is probably lower because some zeros are semi-systematic and you need multiple non-zero counts for the median to work.
Smaller cells will also have more stochastic zeros anyway.
Nonetheless, the equivalent failure point for DESeq would be 50%, which is clearly worse.

## Ensuring identifiability

The use of the library size-sorted ring means that the linear system is not directly solvable when $N$ is not a relative prime with any $s$.
Let $m > 1$ be the greatest common denominator between $N$ and all $s$.
Consider a solution vector for the linear system $\mathbf{a} = (a_1, \ldots, a_N)$ where cells are ordered according to the ring.
We can obtain an equivalent least-squares solution at

\[
\mathbf{a}' = (a_1 + \omega_1, a_2 + \omega_2, \ldots, a_{m-1} + \omega_{m-1}, a_m - \omega_m, a_{m+1} + a_1, \ldots, a_{N-1} + \omega_{m-1}, a_N - \omega_m)
\]

for any values $\omega_1, \ldots, \omega_m$ where $\omega_m = \sum_{i=1}^{m-1} \omega_i$.
Thus, there is no unique solution.

To avoid this problem, we define the pool sizes to be $(21, 26, 31, \ldots, 101)$.
This set of pool sizes includes a number of primes, which reduces the chance that $N$ is not a relative prime with any $s$.

As a fail-safe, we also add a number of linear equations to guarantee solvability.
For each $i$, we set $s_it_i^{-1} = 1/C$.
This is equivalent to performing library size normalization of each cell to the reference pseudo-cell.
These extra equations are set to extremely low weight to avoid having any effect when the system is otherwise fully identifiable.

Note that the extra equations can still lead to the correct solution when the system is not identifiable.
Assume that $m > 1$ such that the exact solution of the linear system is fully dependent on the added equations.
The residual sum of squares for these equations is 

\[
(a_1 + \omega_1 - 1/C)^2 + \ldots + (a_N - \omega_m - 1/C)^2 \;,
\]

where $a_1, \ldots, a_N$ are effectively constant as they are determined by the high-weight equations from the pool.
This is minimized with respect to, say, $\omega_1$ when

\[
\begin{aligned}
&\frac{N}{m}(2\omega_1 + \omega_2 + \ldots + \omega_{m-1}) \\
&+ (a_1 + a_{m+1} + \ldots a_{N - m + 1}) \\
&- (a_m + \ldots + a_N) \\
&=0 \;.
\end{aligned}
\]

If we repeat this for each $\omega$ term, we obtain another linear system that we can solve for $\omega$ (and thus for the least-squares solution).
For $\omega_k$, the right hand side is 

\[
(a_m + \ldots + a_N) - (a_k + a_{m+k} + \ldots a_{N - m + k}) \;,
\]

i.e., differences between the sums of the $a$ terms for every $m$th library on the ring.
This implies that the solution of $\omega$ will aim to minimize any differences between the alternating sums of the final estimates (i.e., of $s_it_i^{-1}$) along the ring.
For example, if the alternating sums are the same in $\mathbf{a}$, we obtain $\omega_1=\omega_2=\ldots=\omega_{m-1}=0$, i.e., the final solution is $\mathbf{a}' = \mathbf{a}$.

It is reasonable to assume that alternating sums are the same for the true $\theta_jt_j^{-1}$.
This is based on the expectation that (i) there should be no systematic structure along _alternating_ positions of the ring, and (ii) $N/m$ is large enough for the sum of every $m$th value to be the same, if not very similar.
This suggests that we will obtain least-squares estimates close to the true values, even when the linear system formed from the pools is not identifiable.
See `simulations/justifications/extraeqn.R` for a demonstration.

## Dealing with negative size factors

A downside of using the linear system is that its solution can theoretically contain negative values.
These do not make sense in the context of scaling normalization.
We previously used `r CRANpkg("limSolve")` to guarantee that the solutions were always non-negative.
However, this was not an ideal strategy as it would return size factor estimates of zero for the affected cells, which was just as useless.

If there are a large number of negative size factors, these are probably due to failure to filter out low-abundance genes.
This results in all pools obtaining $\theta_{P}$ of zero.
The solution is simply to filter out low-abundance genes prior to the deconvolution itself.
`computeSumFactors()` offers the `min.mean` argument to do this, though there are some subtleties with the choice of filtering strategy - see `filtering.Rmd` for details.
Note that the low-abundance genes can still be retained for downstream analyses.
For scaling normalization, it is possible to normalize based on informative high-abundance genes and apply the same size factors to all other genes.

Sporadic negative size factors arise from low-quality cells many zero counts and true size factors near zero.
Errors in the system (due to bias or imprecision) can push these estimates below zero.
It is difficult to accurately normalize such libraries with so little available information.
These are most obviously handled by just filtering those low-quality cells out, e.g., if they do not express many genes.

That said, it may be desirable to keep the affected cells for further analysis, in which case a "reasonable" choice of size factor must be made.
We achieve this in `cleanSizeFactors()` by fitting a curve to the size factors against the number of expressed genes, for all libraries with positive size factors.
This curve has the form

\[
y = \frac{Ax}{Bx+1}
\]

where $y$ is the size factor, $x$ is the number of expressed genes and $A$ and $B$ are coefficients to be estimated.
This is a purely empirical expression, chosen because:

- It passes through the origin. 
The size factor should be zero when there are no genes expressed.
- It is linear near zero.
For a homogeneous population, the scaling bias is usually proportional to the number of expressed genes at low counts.
- It asymptotes at large $x$. 
At high coverage, further increases to the counts should not increase the number of expressed genes.

For any cell with a negative size factor from deconvolution, an alternative is computed from the fitted curve based on the number of expressed genes.
This defines the alternative size factor based on other cells with similar number of expressed genes.
It is more robust than a library size-derived factor to differential expression.

# Further discussion

## Is removal of composition biases really necessary?

Normalizing on library size is usually sufficient for purposes of cell type identification.
Separation of cell types is robust as the presence of differences in the expression profile is not affected by the scaling factor.
Indeed, that a cell type exhibits strong composition biases indicates that it has plenty of DE genes to distinguish it from other cell types.

The main benefit of removing composition biases is to facilitate the intepreration of per-gene results.
Composition biases can shift the log-fold change estimates between clusters or along trajectories, or induce weak artificial correlations between gene pairs.
We can mitigate these effects by normalizing out composition biases, though admittedly, they are usually minor (around 20-50% on top of any existing fold change).
Moreover, DE analyses in practice mostly focus on the top ranking genes, due to issues with interpreting the $p$-values (see `r Biocpkg("simpleSingleCell", "xtra-3b-de.html", label="here")` for a discussion).

In short, removal of composition biases will improve the accuracy of downstream analyses, but is unlikely to have a major effect on the qualitative biological conclusions.
The much bigger effect seems to be whether one chooses to preserve differences due to total RNA content via spike-in normalization [@lun2017assessing].

# References
